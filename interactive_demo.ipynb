{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤– AI-Powered Data Cleaning Agent - Interactive Demo\n",
        "\n",
        "**GenAI Competition - UoM DSCubed x UWA DSC**  \n",
        "**Author:** Rudra Tiwari\n",
        "\n",
        "This interactive notebook demonstrates the AI-Powered Data Cleaning Agent with OpenAI integration. Follow each cell sequentially to see how AI can transform your data cleaning workflow.\n",
        "\n",
        "## ðŸŽ¯ What You'll Learn:\n",
        "- How to load and analyze data quality issues\n",
        "- AI-powered cleaning suggestions using OpenAI GPT-4o-mini\n",
        "- Automated data cleaning with intelligent strategies\n",
        "- Beautiful visualizations and comprehensive reports\n",
        "- Export capabilities for cleaned data\n",
        "\n",
        "## ðŸ“‹ Prerequisites:\n",
        "- OpenAI API Key (get one at [platform.openai.com](https://platform.openai.com/api-keys))\n",
        "- Upload your dataset or use the provided WHO health data example\n",
        "\n",
        "Let's get started! ðŸš€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install Required Packages\n",
        "# Run this cell first to install all necessary dependencies\n",
        "\n",
        "%pip install pandas numpy matplotlib seaborn openpyxl langchain langchain-openai python-dotenv scikit-learn -q\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")\n",
        "print(\"ðŸ“¦ Ready to start the AI Data Cleaning Agent demo!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Import Libraries and Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "import os\n",
        "from io import StringIO\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ðŸ“š Libraries imported successfully!\")\n",
        "print(\"ðŸŽ¨ Plotting style configured!\")\n",
        "print(\"ðŸ”§ Ready to initialize the Data Cleaning Agent!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Initialize the Data Cleaning Agent\n",
        "# This is the core class that handles all data cleaning operations\n",
        "\n",
        "class DataCleaningAgent:\n",
        "    \"\"\"\n",
        "    AI-Powered Data Cleaning Agent\n",
        "    Provides intelligent data cleaning with comprehensive analysis\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.cleaning_history = []\n",
        "        self.data_quality_report = {}\n",
        "        self.cleaning_suggestions = []\n",
        "    \n",
        "    def analyze_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"Comprehensive data quality analysis\"\"\"\n",
        "        print(\"ðŸ” Analyzing Data Quality...\")\n",
        "        \n",
        "        analysis = {\n",
        "            'shape': df.shape,\n",
        "            'columns': list(df.columns),\n",
        "            'data_types': df.dtypes.to_dict(),\n",
        "            'missing_values': df.isnull().sum().to_dict(),\n",
        "            'missing_percentage': (df.isnull().sum() / len(df) * 100).to_dict(),\n",
        "            'duplicate_rows': df.duplicated().sum(),\n",
        "            'duplicate_percentage': (df.duplicated().sum() / len(df) * 100),\n",
        "            'memory_usage': df.memory_usage(deep=True).sum(),\n",
        "            'numeric_columns': df.select_dtypes(include=[np.number]).columns.tolist(),\n",
        "            'categorical_columns': df.select_dtypes(include=['object']).columns.tolist(),\n",
        "            'datetime_columns': df.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "        }\n",
        "        \n",
        "        # Detect potential issues\n",
        "        issues = []\n",
        "        if analysis['missing_percentage']:\n",
        "            high_missing = {col: pct for col, pct in analysis['missing_percentage'].items() if pct > 50}\n",
        "            if high_missing:\n",
        "                issues.append(f\"High missing values (>50%): {high_missing}\")\n",
        "        \n",
        "        if analysis['duplicate_percentage'] > 10:\n",
        "            issues.append(f\"High duplicate rate: {analysis['duplicate_percentage']:.1f}%\")\n",
        "        \n",
        "        analysis['issues'] = issues\n",
        "        self.data_quality_report = analysis\n",
        "        \n",
        "        return analysis\n",
        "\n",
        "# Initialize the agent\n",
        "agent = DataCleaningAgent()\n",
        "print(\"ðŸ¤– Data Cleaning Agent initialized successfully!\")\n",
        "print(\"âœ… Ready to analyze and clean your data!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI-Powered Data Cleaning Agent - Interactive Demo\n",
        "\n",
        "**GenAI Competition - UoM DSCubed x UWA DSC**  \n",
        "**Author:** Rudra Tiwari  \n",
        "**Complete Standalone Data Cleaning Agent**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ **Welcome to the AI-Powered Data Cleaning Agent!**\n",
        "\n",
        "This notebook demonstrates advanced data cleaning capabilities with AI integration. It's designed to work with any Excel file, including the included WHO health dataset.\n",
        "\n",
        "### **Key Features:**\n",
        "- ðŸ¤– **AI-Powered Intelligent Cleaning** - Uses OpenAI API for smart suggestions\n",
        "- ðŸ“Š **Multi-Sheet Excel Support** - Handles complex Excel files\n",
        "- ðŸ” **Comprehensive Data Quality Analysis** - Detailed analysis of data issues\n",
        "- ðŸ“ˆ **Before/After Comparisons** - Visual comparisons of cleaning results\n",
        "- ðŸ“‹ **Professional Reporting** - Generates detailed cleaning reports\n",
        "- ðŸŽ¯ **Real-World Health Data Processing** - Works with WHO health datasets\n",
        "- ðŸ“Š **Beautiful Visualizations** - Interactive dashboards and charts\n",
        "\n",
        "### **Perfect for:**\n",
        "- Data scientists and analysts\n",
        "- Healthcare data processing\n",
        "- Business intelligence\n",
        "- Academic research\n",
        "- Competition demonstrations\n",
        "\n",
        "**Ready to clean some data? Let's get started! ðŸŽ‰**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "%pip install pandas numpy matplotlib seaborn openpyxl langchain langchain-openai ipywidgets scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Import All Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "import json\n",
        "from datetime import datetime\n",
        "import io\n",
        "import base64\n",
        "\n",
        "# AI Libraries\n",
        "try:\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from langchain.schema import HumanMessage, SystemMessage\n",
        "    AI_AVAILABLE = True\n",
        "except ImportError:\n",
        "    AI_AVAILABLE = False\n",
        "    print(\"âš ï¸ AI libraries not available. Install with: pip install langchain langchain-openai\")\n",
        "\n",
        "# Visualization settings\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(f\"ðŸ¤– AI Features Available: {AI_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Set Your OpenAI API Key (Optional)\n",
        "\n",
        "# Set your OpenAI API key here (uncomment and replace with your actual key)\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-your-actual-api-key-here\"\n",
        "\n",
        "# Alternative: Set via environment variable (recommended for security)\n",
        "# In terminal: export OPENAI_API_KEY=\"sk-your-actual-api-key-here\"\n",
        "# In Colab: !export OPENAI_API_KEY=\"sk-your-actual-api-key-here\"\n",
        "\n",
        "# Check if API key is set\n",
        "current_key = os.environ.get(\"OPENAI_API_KEY\", \"not-set\")\n",
        "if current_key != \"not-set\" and current_key != \"your-openai-api-key-here\":\n",
        "    print(\"âœ… OpenAI API key is configured!\")\n",
        "    print(f\"Key starts with: {current_key[:10]}...\")\n",
        "else:\n",
        "    print(\"âš ï¸ OpenAI API key not set. AI features will use fallback suggestions.\")\n",
        "    print(\"To enable AI features, uncomment the line above and add your API key.\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Note: The notebook will work without the API key, but AI suggestions will be limited.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Configuration Settings\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"your-openai-api-key-here\")\n",
        "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "OPENAI_TEMPERATURE = float(os.environ.get(\"OPENAI_TEMPERATURE\", \"0.0\"))\n",
        "\n",
        "# Initialize AI client if available\n",
        "ai_client = None\n",
        "if AI_AVAILABLE and OPENAI_API_KEY != \"your-openai-api-key-here\":\n",
        "    try:\n",
        "        ai_client = ChatOpenAI(\n",
        "            model=OPENAI_MODEL,\n",
        "            temperature=OPENAI_TEMPERATURE,\n",
        "            api_key=OPENAI_API_KEY\n",
        "        )\n",
        "        print(\"âœ… AI client initialized successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Failed to initialize AI client: {e}\")\n",
        "        ai_client = None\n",
        "else:\n",
        "    print(\"âš ï¸ AI client not initialized - using fallback suggestions\")\n",
        "\n",
        "print(f\"ðŸ¤– Model: {OPENAI_MODEL}\")\n",
        "print(f\"ðŸŒ¡ï¸ Temperature: {OPENAI_TEMPERATURE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Core Data Cleaning Agent Class\n",
        "class DataCleaningAgent:\n",
        "    \"\"\"\n",
        "    Advanced Data Cleaning Agent with comprehensive cleaning capabilities\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.cleaning_log = []\n",
        "        self.original_shape = None\n",
        "        self.cleaned_shape = None\n",
        "        \n",
        "    def log_action(self, action: str, details: str = \"\"):\n",
        "        \"\"\"Log cleaning actions for reporting\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        self.cleaning_log.append({\n",
        "            'timestamp': timestamp,\n",
        "            'action': action,\n",
        "            'details': details\n",
        "        })\n",
        "    \n",
        "    def load_excel_multi_sheet(self, file_path: str, sheet_name: str = None) -> pd.DataFrame:\n",
        "        \"\"\"Load Excel file with intelligent sheet detection\"\"\"\n",
        "        try:\n",
        "            # Read all sheet names\n",
        "            excel_file = pd.ExcelFile(file_path)\n",
        "            sheet_names = excel_file.sheet_names\n",
        "            \n",
        "            print(f\"ðŸ“Š Available sheets: {sheet_names}\")\n",
        "            \n",
        "            if sheet_name is None:\n",
        "                # Auto-select the largest sheet with data\n",
        "                best_sheet = None\n",
        "                max_rows = 0\n",
        "                \n",
        "                for sheet in sheet_names:\n",
        "                    try:\n",
        "                        df_test = pd.read_excel(file_path, sheet_name=sheet, header=None)\n",
        "                        if len(df_test) > max_rows:\n",
        "                            max_rows = len(df_test)\n",
        "                            best_sheet = sheet\n",
        "                    except:\n",
        "                        continue\n",
        "                \n",
        "                sheet_name = best_sheet\n",
        "                print(f\"ðŸŽ¯ Auto-selected sheet: {sheet_name}\")\n",
        "            \n",
        "            # Load the selected sheet\n",
        "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "            \n",
        "            # Try to find the best header row\n",
        "            best_header = self._find_best_header(df)\n",
        "            if best_header > 0:\n",
        "                df = pd.read_excel(file_path, sheet_name=sheet_name, header=best_header)\n",
        "                print(f\"ðŸ“‹ Using header row: {best_header}\")\n",
        "            \n",
        "            self.original_shape = df.shape\n",
        "            self.log_action(\"Data Loaded\", f\"Shape: {df.shape}, Sheet: {sheet_name}\")\n",
        "            \n",
        "            return df\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading Excel file: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def _find_best_header(self, df: pd.DataFrame) -> int:\n",
        "        \"\"\"Find the best header row for the dataset\"\"\"\n",
        "        for i in range(min(10, len(df))):\n",
        "            row = df.iloc[i]\n",
        "            # Check if this row looks like headers (mostly strings, few nulls)\n",
        "            string_count = sum(1 for val in row if isinstance(val, str) and val.strip())\n",
        "            null_count = row.isnull().sum()\n",
        "            \n",
        "            if string_count > len(row) * 0.5 and null_count < len(row) * 0.3:\n",
        "                return i\n",
        "        return 0\n",
        "    \n",
        "    def analyze_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"Comprehensive data quality analysis\"\"\"\n",
        "        analysis = {\n",
        "            'shape': df.shape,\n",
        "            'missing_values': df.isnull().sum().to_dict(),\n",
        "            'missing_percentage': (df.isnull().sum() / len(df) * 100).to_dict(),\n",
        "            'duplicate_rows': df.duplicated().sum(),\n",
        "            'data_types': df.dtypes.to_dict(),\n",
        "            'memory_usage': df.memory_usage(deep=True).sum(),\n",
        "            'numeric_columns': df.select_dtypes(include=[np.number]).columns.tolist(),\n",
        "            'categorical_columns': df.select_dtypes(include=['object']).columns.tolist(),\n",
        "            'datetime_columns': df.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "        }\n",
        "        \n",
        "        # Outlier detection for numeric columns\n",
        "        outliers = {}\n",
        "        for col in analysis['numeric_columns']:\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                Q1 = df[col].quantile(0.25)\n",
        "                Q3 = df[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                outlier_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
        "                outliers[col] = outlier_count\n",
        "        \n",
        "        analysis['outliers'] = outliers\n",
        "        \n",
        "        self.log_action(\"Data Quality Analysis\", f\"Found {analysis['duplicate_rows']} duplicates, {sum(analysis['missing_values'].values())} missing values\")\n",
        "        \n",
        "        return analysis\n",
        "    \n",
        "    def clean_missing_values(self, df: pd.DataFrame, strategy: str = 'intelligent') -> pd.DataFrame:\n",
        "        \"\"\"Intelligent missing value imputation\"\"\"\n",
        "        df_cleaned = df.copy()\n",
        "        \n",
        "        for col in df_cleaned.columns:\n",
        "            missing_count = df_cleaned[col].isnull().sum()\n",
        "            if missing_count > 0:\n",
        "                if strategy == 'intelligent':\n",
        "                    if df_cleaned[col].dtype in ['int64', 'float64']:\n",
        "                        # For numeric columns, use median\n",
        "                        df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
        "                    else:\n",
        "                        # For categorical columns, use mode\n",
        "                        mode_value = df_cleaned[col].mode()\n",
        "                        if len(mode_value) > 0:\n",
        "                            df_cleaned[col].fillna(mode_value[0], inplace=True)\n",
        "                        else:\n",
        "                            df_cleaned[col].fillna('Unknown', inplace=True)\n",
        "                elif strategy == 'drop':\n",
        "                    df_cleaned = df_cleaned.dropna(subset=[col])\n",
        "                elif strategy == 'forward_fill':\n",
        "                    df_cleaned[col].fillna(method='ffill', inplace=True)\n",
        "                elif strategy == 'backward_fill':\n",
        "                    df_cleaned[col].fillna(method='bfill', inplace=True)\n",
        "        \n",
        "        self.log_action(\"Missing Values Cleaned\", f\"Strategy: {strategy}\")\n",
        "        return df_cleaned\n",
        "    \n",
        "    def remove_duplicates(self, df: pd.DataFrame, subset: List[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"Remove duplicate rows\"\"\"\n",
        "        initial_count = len(df)\n",
        "        df_cleaned = df.drop_duplicates(subset=subset, keep='first')\n",
        "        removed_count = initial_count - len(df_cleaned)\n",
        "        \n",
        "        self.log_action(\"Duplicates Removed\", f\"Removed {removed_count} duplicate rows\")\n",
        "        return df_cleaned\n",
        "    \n",
        "    def standardize_data_types(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Optimize data types for memory efficiency\"\"\"\n",
        "        df_cleaned = df.copy()\n",
        "        \n",
        "        for col in df_cleaned.columns:\n",
        "            if df_cleaned[col].dtype == 'object':\n",
        "                # Try to convert to numeric\n",
        "                try:\n",
        "                    df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='ignore')\n",
        "                except:\n",
        "                    pass\n",
        "                \n",
        "                # Try to convert to datetime\n",
        "                if df_cleaned[col].dtype == 'object':\n",
        "                    try:\n",
        "                        df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='ignore')\n",
        "                    except:\n",
        "                        pass\n",
        "            \n",
        "            # Optimize numeric types\n",
        "            if df_cleaned[col].dtype in ['int64', 'float64']:\n",
        "                if df_cleaned[col].dtype == 'int64':\n",
        "                    if df_cleaned[col].min() >= 0:\n",
        "                        if df_cleaned[col].max() < 255:\n",
        "                            df_cleaned[col] = df_cleaned[col].astype('uint8')\n",
        "                        elif df_cleaned[col].max() < 65535:\n",
        "                            df_cleaned[col] = df_cleaned[col].astype('uint16')\n",
        "                        elif df_cleaned[col].max() < 4294967295:\n",
        "                            df_cleaned[col] = df_cleaned[col].astype('uint32')\n",
        "                    else:\n",
        "                        if df_cleaned[col].min() > -128 and df_cleaned[col].max() < 127:\n",
        "                            df_cleaned[col] = df_cleaned[col].astype('int8')\n",
        "                        elif df_cleaned[col].min() > -32768 and df_cleaned[col].max() < 32767:\n",
        "                            df_cleaned[col] = df_cleaned[col].astype('int16')\n",
        "                        elif df_cleaned[col].min() > -2147483648 and df_cleaned[col].max() < 2147483647:\n",
        "                            df_cleaned[col] = df_cleaned[col].astype('int32')\n",
        "                \n",
        "                elif df_cleaned[col].dtype == 'float64':\n",
        "                    df_cleaned[col] = pd.to_numeric(df_cleaned[col], downcast='float')\n",
        "        \n",
        "        self.log_action(\"Data Types Optimized\", \"Memory usage reduced\")\n",
        "        return df_cleaned\n",
        "    \n",
        "    def detect_outliers(self, df: pd.DataFrame, columns: List[str] = None) -> Dict[str, List[int]]:\n",
        "        \"\"\"Detect outliers using IQR method\"\"\"\n",
        "        if columns is None:\n",
        "            columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        \n",
        "        outliers = {}\n",
        "        for col in columns:\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                Q1 = df[col].quantile(0.25)\n",
        "                Q3 = df[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                \n",
        "                outlier_indices = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index.tolist()\n",
        "                outliers[col] = outlier_indices\n",
        "        \n",
        "        return outliers\n",
        "    \n",
        "    def clean_outliers(self, df: pd.DataFrame, method: str = 'cap', columns: List[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"Clean outliers using various methods\"\"\"\n",
        "        df_cleaned = df.copy()\n",
        "        \n",
        "        if columns is None:\n",
        "            columns = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        \n",
        "        for col in columns:\n",
        "            if df_cleaned[col].dtype in ['int64', 'float64']:\n",
        "                Q1 = df_cleaned[col].quantile(0.25)\n",
        "                Q3 = df_cleaned[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                \n",
        "                if method == 'cap':\n",
        "                    df_cleaned[col] = df_cleaned[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "                elif method == 'remove':\n",
        "                    df_cleaned = df_cleaned[(df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound)]\n",
        "                elif method == 'median':\n",
        "                    median_value = df_cleaned[col].median()\n",
        "                    df_cleaned.loc[(df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound), col] = median_value\n",
        "        \n",
        "        self.log_action(\"Outliers Cleaned\", f\"Method: {method}\")\n",
        "        return df_cleaned\n",
        "    \n",
        "    def standardize_text(self, df: pd.DataFrame, columns: List[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"Standardize text data\"\"\"\n",
        "        df_cleaned = df.copy()\n",
        "        \n",
        "        if columns is None:\n",
        "            columns = df_cleaned.select_dtypes(include=['object']).columns.tolist()\n",
        "        \n",
        "        for col in columns:\n",
        "            if df_cleaned[col].dtype == 'object':\n",
        "                # Remove extra whitespace\n",
        "                df_cleaned[col] = df_cleaned[col].astype(str).str.strip()\n",
        "                # Convert to title case\n",
        "                df_cleaned[col] = df_cleaned[col].str.title()\n",
        "                # Replace multiple spaces with single space\n",
        "                df_cleaned[col] = df_cleaned[col].str.replace(r'\\s+', ' ', regex=True)\n",
        "        \n",
        "        self.log_action(\"Text Standardized\", f\"Columns: {len(columns)}\")\n",
        "        return df_cleaned\n",
        "    \n",
        "    def auto_clean(self, df: pd.DataFrame, \n",
        "                   clean_missing: bool = True,\n",
        "                   remove_duplicates: bool = True,\n",
        "                   standardize_types: bool = True,\n",
        "                   clean_outliers: bool = False,\n",
        "                   standardize_text: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"Automated cleaning pipeline\"\"\"\n",
        "        df_cleaned = df.copy()\n",
        "        \n",
        "        print(\"ðŸ§¹ Starting automated cleaning pipeline...\")\n",
        "        \n",
        "        if clean_missing:\n",
        "            print(\"  ðŸ“ Cleaning missing values...\")\n",
        "            df_cleaned = self.clean_missing_values(df_cleaned, strategy='intelligent')\n",
        "        \n",
        "        if remove_duplicates:\n",
        "            print(\"  ðŸ”„ Removing duplicates...\")\n",
        "            df_cleaned = self.remove_duplicates(df_cleaned)\n",
        "        \n",
        "        if standardize_types:\n",
        "            print(\"  ðŸ”§ Standardizing data types...\")\n",
        "            df_cleaned = self.standardize_data_types(df_cleaned)\n",
        "        \n",
        "        if clean_outliers:\n",
        "            print(\"  ðŸ“Š Cleaning outliers...\")\n",
        "            df_cleaned = self.clean_outliers(df_cleaned, method='cap')\n",
        "        \n",
        "        if standardize_text:\n",
        "            print(\"  âœï¸ Standardizing text...\")\n",
        "            df_cleaned = self.standardize_text(df_cleaned)\n",
        "        \n",
        "        self.cleaned_shape = df_cleaned.shape\n",
        "        self.log_action(\"Auto Clean Complete\", f\"Final shape: {df_cleaned.shape}\")\n",
        "        \n",
        "        print(\"âœ… Automated cleaning completed!\")\n",
        "        return df_cleaned\n",
        "    \n",
        "    def generate_report(self) -> str:\n",
        "        \"\"\"Generate comprehensive cleaning report\"\"\"\n",
        "        report = f\"\"\"\n",
        "# Data Cleaning Report\n",
        "Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "\n",
        "## Summary\n",
        "- **Original Shape:** {self.original_shape}\n",
        "- **Cleaned Shape:** {self.cleaned_shape}\n",
        "- **Rows Removed:** {self.original_shape[0] - self.cleaned_shape[0] if self.cleaned_shape else 0}\n",
        "- **Columns:** {self.cleaned_shape[1] if self.cleaned_shape else 0}\n",
        "\n",
        "## Cleaning Actions Performed\n",
        "\"\"\"\n",
        "        \n",
        "        for log_entry in self.cleaning_log:\n",
        "            report += f\"- **{log_entry['timestamp']}:** {log_entry['action']}\"\n",
        "            if log_entry['details']:\n",
        "                report += f\" - {log_entry['details']}\"\n",
        "            report += \"\\n\"\n",
        "        \n",
        "        return report\n",
        "\n",
        "print(\"âœ… DataCleaningAgent class loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: AI-Powered Data Cleaning Agent\n",
        "class AIDataCleaningAgent(DataCleaningAgent):\n",
        "    \"\"\"\n",
        "    AI-Enhanced Data Cleaning Agent with OpenAI integration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, ai_client=None):\n",
        "        super().__init__()\n",
        "        self.ai_client = ai_client\n",
        "        self.ai_suggestions = []\n",
        "    \n",
        "    def get_ai_cleaning_suggestions(self, df: pd.DataFrame, analysis: Dict[str, Any]) -> List[Dict[str, str]]:\n",
        "        \"\"\"Get AI-powered cleaning suggestions\"\"\"\n",
        "        if not self.ai_client:\n",
        "            return self._get_fallback_suggestions(df, analysis)\n",
        "        \n",
        "        try:\n",
        "            # Prepare data summary for AI\n",
        "            data_summary = f\"\"\"\n",
        "            Dataset Summary:\n",
        "            - Shape: {df.shape}\n",
        "            - Missing values: {sum(analysis['missing_values'].values())}\n",
        "            - Duplicate rows: {analysis['duplicate_rows']}\n",
        "            - Numeric columns: {len(analysis['numeric_columns'])}\n",
        "            - Categorical columns: {len(analysis['categorical_columns'])}\n",
        "            - Memory usage: {analysis['memory_usage']} bytes\n",
        "            \n",
        "            Column details:\n",
        "            \"\"\"\n",
        "            \n",
        "            for col in df.columns:\n",
        "                missing_pct = analysis['missing_percentage'][col]\n",
        "                data_type = str(df[col].dtype)\n",
        "                data_summary += f\"- {col}: {data_type}, {missing_pct:.1f}% missing\\n\"\n",
        "            \n",
        "            # Create AI prompt\n",
        "            system_prompt = \"\"\"You are an expert data cleaning specialist. Analyze the dataset and provide specific, actionable cleaning suggestions. \n",
        "            Focus on practical steps that will improve data quality. Be concise and specific.\"\"\"\n",
        "            \n",
        "            user_prompt = f\"\"\"Please analyze this dataset and provide 3-5 specific cleaning recommendations:\n",
        "            \n",
        "            {data_summary}\n",
        "            \n",
        "            Provide suggestions in this format:\n",
        "            1. [Action]: [Description] - [Reason]\n",
        "            2. [Action]: [Description] - [Reason]\n",
        "            etc.\n",
        "            \"\"\"\n",
        "            \n",
        "            messages = [\n",
        "                SystemMessage(content=system_prompt),\n",
        "                HumanMessage(content=user_prompt)\n",
        "            ]\n",
        "            \n",
        "            response = self.ai_client.invoke(messages)\n",
        "            suggestions_text = response.content\n",
        "            \n",
        "            # Parse suggestions\n",
        "            suggestions = []\n",
        "            for line in suggestions_text.split('\\n'):\n",
        "                if line.strip() and (line.strip().startswith(('1.', '2.', '3.', '4.', '5.'))):\n",
        "                    parts = line.split(':', 2)\n",
        "                    if len(parts) >= 2:\n",
        "                        action = parts[1].split('-')[0].strip()\n",
        "                        reason = parts[1].split('-')[1].strip() if '-' in parts[1] else \"Improves data quality\"\n",
        "                        suggestions.append({\n",
        "                            'action': action,\n",
        "                            'reason': reason,\n",
        "                            'priority': 'high' if 'missing' in action.lower() or 'duplicate' in action.lower() else 'medium'\n",
        "                        })\n",
        "            \n",
        "            self.ai_suggestions = suggestions\n",
        "            self.log_action(\"AI Suggestions Generated\", f\"Generated {len(suggestions)} suggestions\")\n",
        "            \n",
        "            return suggestions\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ AI suggestion generation failed: {e}\")\n",
        "            return self._get_fallback_suggestions(df, analysis)\n",
        "    \n",
        "    def _get_fallback_suggestions(self, df: pd.DataFrame, analysis: Dict[str, Any]) -> List[Dict[str, str]]:\n",
        "        \"\"\"Fallback suggestions when AI is not available\"\"\"\n",
        "        suggestions = []\n",
        "        \n",
        "        # Check for missing values\n",
        "        missing_cols = [col for col, count in analysis['missing_values'].items() if count > 0]\n",
        "        if missing_cols:\n",
        "            suggestions.append({\n",
        "                'action': f\"Clean missing values in {len(missing_cols)} columns\",\n",
        "                'reason': f\"Found missing values in: {', '.join(missing_cols[:3])}{'...' if len(missing_cols) > 3 else ''}\",\n",
        "                'priority': 'high'\n",
        "            })\n",
        "        \n",
        "        # Check for duplicates\n",
        "        if analysis['duplicate_rows'] > 0:\n",
        "            suggestions.append({\n",
        "                'action': f\"Remove {analysis['duplicate_rows']} duplicate rows\",\n",
        "                'reason': \"Duplicate rows can skew analysis results\",\n",
        "                'priority': 'high'\n",
        "            })\n",
        "        \n",
        "        # Check for outliers\n",
        "        outlier_cols = [col for col, count in analysis['outliers'].items() if count > 0]\n",
        "        if outlier_cols:\n",
        "            suggestions.append({\n",
        "                'action': f\"Review outliers in {len(outlier_cols)} numeric columns\",\n",
        "                'reason': f\"Outliers detected in: {', '.join(outlier_cols[:3])}{'...' if len(outlier_cols) > 3 else ''}\",\n",
        "                'priority': 'medium'\n",
        "            })\n",
        "        \n",
        "        # Check data types\n",
        "        object_cols = analysis['categorical_columns']\n",
        "        if object_cols:\n",
        "            suggestions.append({\n",
        "                'action': f\"Standardize text in {len(object_cols)} categorical columns\",\n",
        "                'reason': \"Text standardization improves consistency\",\n",
        "                'priority': 'medium'\n",
        "            })\n",
        "        \n",
        "        # Memory optimization\n",
        "        if analysis['memory_usage'] > 1000000:  # > 1MB\n",
        "            suggestions.append({\n",
        "                'action': \"Optimize data types for memory efficiency\",\n",
        "                'reason': f\"Current memory usage: {analysis['memory_usage']/1024/1024:.1f}MB\",\n",
        "                'priority': 'low'\n",
        "            })\n",
        "        \n",
        "        self.ai_suggestions = suggestions\n",
        "        return suggestions\n",
        "    \n",
        "    def intelligent_clean(self, df: pd.DataFrame, \n",
        "                         follow_ai_suggestions: bool = True,\n",
        "                         custom_actions: List[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"Intelligent cleaning based on AI suggestions\"\"\"\n",
        "        print(\"ðŸ¤– Starting AI-powered intelligent cleaning...\")\n",
        "        \n",
        "        # Get data quality analysis\n",
        "        analysis = self.analyze_data_quality(df)\n",
        "        \n",
        "        # Get AI suggestions\n",
        "        suggestions = self.get_ai_cleaning_suggestions(df, analysis)\n",
        "        \n",
        "        print(f\"ðŸ’¡ AI Generated {len(suggestions)} cleaning suggestions:\")\n",
        "        for i, suggestion in enumerate(suggestions, 1):\n",
        "            priority_emoji = \"ðŸ”´\" if suggestion['priority'] == 'high' else \"ðŸŸ¡\" if suggestion['priority'] == 'medium' else \"ðŸŸ¢\"\n",
        "            print(f\"  {i}. {priority_emoji} {suggestion['action']}\")\n",
        "            print(f\"     Reason: {suggestion['reason']}\")\n",
        "        \n",
        "        # Apply cleaning based on suggestions\n",
        "        df_cleaned = df.copy()\n",
        "        \n",
        "        if follow_ai_suggestions:\n",
        "            # Apply high priority suggestions automatically\n",
        "            for suggestion in suggestions:\n",
        "                if suggestion['priority'] == 'high':\n",
        "                    action = suggestion['action'].lower()\n",
        "                    \n",
        "                    if 'missing values' in action:\n",
        "                        print(\"  ðŸ§¹ Applying missing value cleaning...\")\n",
        "                        df_cleaned = self.clean_missing_values(df_cleaned, strategy='intelligent')\n",
        "                    \n",
        "                    elif 'duplicate' in action:\n",
        "                        print(\"  ðŸ§¹ Removing duplicates...\")\n",
        "                        df_cleaned = self.remove_duplicates(df_cleaned)\n",
        "                    \n",
        "                    elif 'standardize' in action and 'text' in action:\n",
        "                        print(\"  ðŸ§¹ Standardizing text...\")\n",
        "                        df_cleaned = self.standardize_text(df_cleaned)\n",
        "        \n",
        "        # Apply custom actions if provided\n",
        "        if custom_actions:\n",
        "            for action in custom_actions:\n",
        "                if action == 'optimize_types':\n",
        "                    print(\"  ðŸ§¹ Optimizing data types...\")\n",
        "                    df_cleaned = self.standardize_data_types(df_cleaned)\n",
        "                elif action == 'clean_outliers':\n",
        "                    print(\"  ðŸ§¹ Cleaning outliers...\")\n",
        "                    df_cleaned = self.clean_outliers(df_cleaned, method='cap')\n",
        "        \n",
        "        self.cleaned_shape = df_cleaned.shape\n",
        "        self.log_action(\"AI Intelligent Clean Complete\", f\"Applied {len(suggestions)} suggestions\")\n",
        "        \n",
        "        print(\"âœ… AI-powered cleaning completed!\")\n",
        "        return df_cleaned\n",
        "    \n",
        "    def generate_ai_report(self) -> str:\n",
        "        \"\"\"Generate AI-enhanced cleaning report\"\"\"\n",
        "        base_report = self.generate_report()\n",
        "        \n",
        "        ai_section = f\"\"\"\n",
        "## AI Analysis & Suggestions\n",
        "Generated {len(self.ai_suggestions)} intelligent suggestions:\n",
        "\n",
        "\"\"\"\n",
        "        \n",
        "        for i, suggestion in enumerate(self.ai_suggestions, 1):\n",
        "            priority_emoji = \"ðŸ”´\" if suggestion['priority'] == 'high' else \"ðŸŸ¡\" if suggestion['priority'] == 'medium' else \"ðŸŸ¢\"\n",
        "            ai_section += f\"{i}. {priority_emoji} **{suggestion['action']}**\\n\"\n",
        "            ai_section += f\"   - Reason: {suggestion['reason']}\\n\"\n",
        "            ai_section += f\"   - Priority: {suggestion['priority'].title()}\\n\\n\"\n",
        "        \n",
        "        return base_report + ai_section\n",
        "\n",
        "print(\"âœ… AIDataCleaningAgent class loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Visualization and UI Components\n",
        "class DataCleaningUI:\n",
        "    \"\"\"\n",
        "    Interactive UI components for data cleaning visualization\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_visualization_dashboard(df: pd.DataFrame, analysis: Dict[str, Any]) -> None:\n",
        "        \"\"\"Create comprehensive visualization dashboard\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('ðŸ“Š Data Quality Analysis Dashboard', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. Missing Values Heatmap\n",
        "        missing_data = df.isnull().sum()\n",
        "        missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
        "        \n",
        "        if len(missing_data) > 0:\n",
        "            axes[0, 0].bar(range(len(missing_data)), missing_data.values)\n",
        "            axes[0, 0].set_title('Missing Values by Column')\n",
        "            axes[0, 0].set_xlabel('Columns')\n",
        "            axes[0, 0].set_ylabel('Missing Count')\n",
        "            axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "            if len(missing_data) <= 10:\n",
        "                axes[0, 0].set_xticks(range(len(missing_data)))\n",
        "                axes[0, 0].set_xticklabels(missing_data.index, rotation=45)\n",
        "        else:\n",
        "            axes[0, 0].text(0.5, 0.5, 'âœ… No Missing Values!', \n",
        "                           ha='center', va='center', fontsize=14, color='green')\n",
        "            axes[0, 0].set_title('Missing Values by Column')\n",
        "        \n",
        "        # 2. Data Types Distribution\n",
        "        dtype_counts = df.dtypes.value_counts()\n",
        "        axes[0, 1].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%')\n",
        "        axes[0, 1].set_title('Data Types Distribution')\n",
        "        \n",
        "        # 3. Dataset Shape Info\n",
        "        shape_info = f\"Rows: {df.shape[0]:,}\\nColumns: {df.shape[1]}\\nMemory: {df.memory_usage(deep=True).sum()/1024/1024:.1f} MB\"\n",
        "        axes[0, 2].text(0.1, 0.5, shape_info, fontsize=12, va='center',\n",
        "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
        "        axes[0, 2].set_title('Dataset Information')\n",
        "        axes[0, 2].axis('off')\n",
        "        \n",
        "        # 4. Numeric Columns Distribution (if any)\n",
        "        numeric_cols = analysis['numeric_columns']\n",
        "        if len(numeric_cols) > 0:\n",
        "            # Show distribution of first numeric column\n",
        "            col = numeric_cols[0]\n",
        "            axes[1, 0].hist(df[col].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
        "            axes[1, 0].set_title(f'Distribution of {col}')\n",
        "            axes[1, 0].set_xlabel(col)\n",
        "            axes[1, 0].set_ylabel('Frequency')\n",
        "        else:\n",
        "            axes[1, 0].text(0.5, 0.5, 'No Numeric Columns', \n",
        "                           ha='center', va='center', fontsize=14)\n",
        "            axes[1, 0].set_title('Numeric Distribution')\n",
        "        \n",
        "        # 5. Categorical Columns (if any)\n",
        "        categorical_cols = analysis['categorical_columns']\n",
        "        if len(categorical_cols) > 0:\n",
        "            # Show top values of first categorical column\n",
        "            col = categorical_cols[0]\n",
        "            top_values = df[col].value_counts().head(10)\n",
        "            axes[1, 1].bar(range(len(top_values)), top_values.values)\n",
        "            axes[1, 1].set_title(f'Top Values in {col}')\n",
        "            axes[1, 1].set_xlabel('Values')\n",
        "            axes[1, 1].set_ylabel('Count')\n",
        "            axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "            if len(top_values) <= 10:\n",
        "                axes[1, 1].set_xticks(range(len(top_values)))\n",
        "                axes[1, 1].set_xticklabels(top_values.index, rotation=45)\n",
        "        else:\n",
        "            axes[1, 1].text(0.5, 0.5, 'No Categorical Columns', \n",
        "                           ha='center', va='center', fontsize=14)\n",
        "            axes[1, 1].set_title('Categorical Analysis')\n",
        "        \n",
        "        # 6. Quality Score\n",
        "        total_cells = df.shape[0] * df.shape[1]\n",
        "        missing_cells = df.isnull().sum().sum()\n",
        "        duplicate_rows = analysis['duplicate_rows']\n",
        "        quality_score = max(0, 100 - (missing_cells/total_cells*100) - (duplicate_rows/df.shape[0]*100))\n",
        "        \n",
        "        axes[1, 2].pie([quality_score, 100-quality_score], \n",
        "                      labels=['Quality', 'Issues'], \n",
        "                      colors=['lightgreen', 'lightcoral'],\n",
        "                      autopct='%1.1f%%')\n",
        "        axes[1, 2].set_title(f'Data Quality Score: {quality_score:.1f}%')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print summary statistics\n",
        "        print(f\"\\nðŸ“ˆ Data Quality Summary:\")\n",
        "        print(f\"   â€¢ Total Rows: {df.shape[0]:,}\")\n",
        "        print(f\"   â€¢ Total Columns: {df.shape[1]}\")\n",
        "        print(f\"   â€¢ Missing Values: {missing_cells:,} ({missing_cells/total_cells*100:.1f}%)\")\n",
        "        print(f\"   â€¢ Duplicate Rows: {duplicate_rows:,} ({duplicate_rows/df.shape[0]*100:.1f}%)\")\n",
        "        print(f\"   â€¢ Quality Score: {quality_score:.1f}%\")\n",
        "        print(f\"   â€¢ Memory Usage: {df.memory_usage(deep=True).sum()/1024/1024:.1f} MB\")\n",
        "    \n",
        "    @staticmethod\n",
        "    def compare_before_after(df_before: pd.DataFrame, df_after: pd.DataFrame, \n",
        "                           analysis_before: Dict[str, Any], analysis_after: Dict[str, Any]) -> None:\n",
        "        \"\"\"Create before/after comparison visualization\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('ðŸ”„ Before vs After Data Cleaning Comparison', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. Shape comparison\n",
        "        shapes = ['Before', 'After']\n",
        "        rows = [df_before.shape[0], df_after.shape[0]]\n",
        "        cols = [df_before.shape[1], df_after.shape[1]]\n",
        "        \n",
        "        x = np.arange(len(shapes))\n",
        "        width = 0.35\n",
        "        \n",
        "        axes[0, 0].bar(x - width/2, rows, width, label='Rows', alpha=0.8)\n",
        "        axes[0, 0].bar(x + width/2, cols, width, label='Columns', alpha=0.8)\n",
        "        axes[0, 0].set_title('Dataset Shape')\n",
        "        axes[0, 0].set_ylabel('Count')\n",
        "        axes[0, 0].set_xticks(x)\n",
        "        axes[0, 0].set_xticklabels(shapes)\n",
        "        axes[0, 0].legend()\n",
        "        \n",
        "        # 2. Missing values comparison\n",
        "        missing_before = sum(analysis_before['missing_values'].values())\n",
        "        missing_after = sum(analysis_after['missing_values'].values())\n",
        "        \n",
        "        axes[0, 1].bar(['Before', 'After'], [missing_before, missing_after], \n",
        "                      color=['lightcoral', 'lightgreen'], alpha=0.8)\n",
        "        axes[0, 1].set_title('Missing Values')\n",
        "        axes[0, 1].set_ylabel('Count')\n",
        "        \n",
        "        # 3. Duplicate rows comparison\n",
        "        dup_before = analysis_before['duplicate_rows']\n",
        "        dup_after = analysis_after['duplicate_rows']\n",
        "        \n",
        "        axes[1, 0].bar(['Before', 'After'], [dup_before, dup_after], \n",
        "                      color=['lightcoral', 'lightgreen'], alpha=0.8)\n",
        "        axes[1, 0].set_title('Duplicate Rows')\n",
        "        axes[1, 0].set_ylabel('Count')\n",
        "        \n",
        "        # 4. Memory usage comparison\n",
        "        mem_before = analysis_before['memory_usage'] / 1024 / 1024\n",
        "        mem_after = analysis_after['memory_usage'] / 1024 / 1024\n",
        "        \n",
        "        axes[1, 1].bar(['Before', 'After'], [mem_before, mem_after], \n",
        "                      color=['lightcoral', 'lightgreen'], alpha=0.8)\n",
        "        axes[1, 1].set_title('Memory Usage')\n",
        "        axes[1, 1].set_ylabel('MB')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print improvement summary\n",
        "        print(f\"\\nðŸŽ¯ Cleaning Results Summary:\")\n",
        "        print(f\"   â€¢ Rows: {df_before.shape[0]:,} â†’ {df_after.shape[0]:,} ({df_before.shape[0] - df_after.shape[0]:,} removed)\")\n",
        "        print(f\"   â€¢ Missing Values: {missing_before:,} â†’ {missing_after:,} ({missing_before - missing_after:,} cleaned)\")\n",
        "        print(f\"   â€¢ Duplicates: {dup_before:,} â†’ {dup_after:,} ({dup_before - dup_after:,} removed)\")\n",
        "        print(f\"   â€¢ Memory: {mem_before:.1f}MB â†’ {mem_after:.1f}MB ({((mem_before-mem_after)/mem_before*100):.1f}% reduction)\")\n",
        "    \n",
        "    @staticmethod\n",
        "    def show_cleaning_options() -> Dict[str, bool]:\n",
        "        \"\"\"Display interactive cleaning options\"\"\"\n",
        "        print(\"ðŸ”§ Data Cleaning Options:\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        options = {\n",
        "            'clean_missing': True,\n",
        "            'remove_duplicates': True,\n",
        "            'standardize_types': True,\n",
        "            'clean_outliers': False,\n",
        "            'standardize_text': True\n",
        "        }\n",
        "        \n",
        "        print(\"Default cleaning pipeline will:\")\n",
        "        print(\"âœ… Clean missing values (intelligent imputation)\")\n",
        "        print(\"âœ… Remove duplicate rows\")\n",
        "        print(\"âœ… Optimize data types\")\n",
        "        print(\"âŒ Clean outliers (optional)\")\n",
        "        print(\"âœ… Standardize text data\")\n",
        "        \n",
        "        print(\"\\nðŸ’¡ You can modify these options in the cleaning function calls.\")\n",
        "        return options\n",
        "\n",
        "print(\"âœ… DataCleaningUI class loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Initialize the Data Cleaning Agents\n",
        "print(\"ðŸš€ Initializing Data Cleaning Agents...\")\n",
        "\n",
        "# Initialize the core cleaning agent\n",
        "cleaning_agent = DataCleaningAgent()\n",
        "\n",
        "# Initialize the AI-enhanced cleaning agent\n",
        "ai_cleaning_agent = AIDataCleaningAgent(ai_client=ai_client)\n",
        "\n",
        "# Initialize the UI components\n",
        "ui = DataCleaningUI()\n",
        "\n",
        "print(\"âœ… All agents initialized successfully!\")\n",
        "print(f\"ðŸ¤– AI Features: {'Enabled' if ai_client else 'Fallback Mode'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Load Your Dataset\n",
        "print(\"ðŸ“ Loading Dataset...\")\n",
        "\n",
        "# Option 1: Load the included WHO Data.xlsx file\n",
        "try:\n",
        "    df = cleaning_agent.load_excel_multi_sheet('WHO Data.xlsx')\n",
        "    if df is not None:\n",
        "        print(\"âœ… WHO Data.xlsx loaded successfully!\")\n",
        "        print(f\"ðŸ“Š Dataset shape: {df.shape}\")\n",
        "        print(f\"ðŸ“‹ Columns: {list(df.columns)}\")\n",
        "    else:\n",
        "        print(\"âŒ Failed to load WHO Data.xlsx\")\n",
        "        df = None\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading WHO Data.xlsx: {e}\")\n",
        "    df = None\n",
        "\n",
        "# Option 2: If you want to upload your own file (uncomment the lines below)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     if filename.endswith('.xlsx') or filename.endswith('.xls'):\n",
        "#         df = cleaning_agent.load_excel_multi_sheet(filename)\n",
        "#         break\n",
        "\n",
        "# Option 3: Create sample data for demonstration (if no file is available)\n",
        "if df is None:\n",
        "    print(\"ðŸ“ Creating sample dataset for demonstration...\")\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Create sample health data\n",
        "    n_samples = 1000\n",
        "    df = pd.DataFrame({\n",
        "        'Patient_ID': range(1, n_samples + 1),\n",
        "        'Age': np.random.normal(45, 15, n_samples).astype(int),\n",
        "        'Gender': np.random.choice(['Male', 'Female', 'Other'], n_samples),\n",
        "        'Blood_Pressure_Systolic': np.random.normal(120, 20, n_samples).astype(int),\n",
        "        'Blood_Pressure_Diastolic': np.random.normal(80, 15, n_samples).astype(int),\n",
        "        'Cholesterol': np.random.normal(200, 50, n_samples).astype(int),\n",
        "        'BMI': np.random.normal(25, 5, n_samples),\n",
        "        'Smoking_Status': np.random.choice(['Never', 'Former', 'Current'], n_samples),\n",
        "        'Exercise_Frequency': np.random.choice(['None', 'Light', 'Moderate', 'Heavy'], n_samples),\n",
        "        'Diabetes_Status': np.random.choice(['No', 'Pre-diabetes', 'Type 1', 'Type 2'], n_samples),\n",
        "        'Heart_Disease_Risk': np.random.choice(['Low', 'Medium', 'High'], n_samples)\n",
        "    })\n",
        "    \n",
        "    # Introduce some data quality issues\n",
        "    # Missing values\n",
        "    missing_indices = np.random.choice(df.index, size=100, replace=False)\n",
        "    df.loc[missing_indices, 'Cholesterol'] = np.nan\n",
        "    \n",
        "    missing_indices = np.random.choice(df.index, size=50, replace=False)\n",
        "    df.loc[missing_indices, 'BMI'] = np.nan\n",
        "    \n",
        "    # Duplicates\n",
        "    duplicate_rows = df.sample(n=20)\n",
        "    df = pd.concat([df, duplicate_rows], ignore_index=True)\n",
        "    \n",
        "    # Outliers\n",
        "    outlier_indices = np.random.choice(df.index, size=10, replace=False)\n",
        "    df.loc[outlier_indices, 'Age'] = np.random.choice([150, 200, 250], size=10)\n",
        "    \n",
        "    # Text inconsistencies\n",
        "    df.loc[df['Gender'] == 'Male', 'Gender'] = 'male'\n",
        "    df.loc[df['Gender'] == 'Female', 'Gender'] = 'female'\n",
        "    \n",
        "    print(\"âœ… Sample dataset created successfully!\")\n",
        "    print(f\"ðŸ“Š Dataset shape: {df.shape}\")\n",
        "    print(f\"ðŸ“‹ Columns: {list(df.columns)}\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Ready to analyze and clean your data!\")\n",
        "print(f\"ðŸ“Š Current dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 10: Data Quality Analysis\n",
        "print(\"ðŸ” Performing Comprehensive Data Quality Analysis...\")\n",
        "\n",
        "# Analyze the dataset\n",
        "analysis = cleaning_agent.analyze_data_quality(df)\n",
        "\n",
        "# Display the analysis results\n",
        "print(f\"\\nðŸ“Š Data Quality Analysis Results:\")\n",
        "print(f\"   â€¢ Dataset Shape: {analysis['shape']}\")\n",
        "print(f\"   â€¢ Missing Values: {sum(analysis['missing_values'].values()):,}\")\n",
        "print(f\"   â€¢ Duplicate Rows: {analysis['duplicate_rows']:,}\")\n",
        "print(f\"   â€¢ Memory Usage: {analysis['memory_usage']/1024/1024:.1f} MB\")\n",
        "print(f\"   â€¢ Numeric Columns: {len(analysis['numeric_columns'])}\")\n",
        "print(f\"   â€¢ Categorical Columns: {len(analysis['categorical_columns'])}\")\n",
        "\n",
        "# Show missing values details\n",
        "missing_cols = [col for col, count in analysis['missing_values'].items() if count > 0]\n",
        "if missing_cols:\n",
        "    print(f\"\\nâŒ Columns with Missing Values:\")\n",
        "    for col in missing_cols:\n",
        "        missing_pct = analysis['missing_percentage'][col]\n",
        "        print(f\"   â€¢ {col}: {analysis['missing_values'][col]:,} ({missing_pct:.1f}%)\")\n",
        "else:\n",
        "    print(f\"\\nâœ… No Missing Values Found!\")\n",
        "\n",
        "# Show outlier information\n",
        "outlier_cols = [col for col, count in analysis['outliers'].items() if count > 0]\n",
        "if outlier_cols:\n",
        "    print(f\"\\nâš ï¸ Columns with Outliers:\")\n",
        "    for col in outlier_cols:\n",
        "        print(f\"   â€¢ {col}: {analysis['outliers'][col]:,} outliers\")\n",
        "else:\n",
        "    print(f\"\\nâœ… No Significant Outliers Found!\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Data Quality Score: {max(0, 100 - (sum(analysis['missing_values'].values())/(analysis['shape'][0]*analysis['shape'][1])*100) - (analysis['duplicate_rows']/analysis['shape'][0]*100)):.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 11: Create Data Quality Visualization Dashboard\n",
        "print(\"ðŸ“Š Creating Data Quality Visualization Dashboard...\")\n",
        "\n",
        "# Create the comprehensive dashboard\n",
        "ui.create_visualization_dashboard(df, analysis)\n",
        "\n",
        "print(\"âœ… Visualization dashboard created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 12: AI-Powered Cleaning Suggestions\n",
        "print(\"ðŸ¤– Getting AI-Powered Cleaning Suggestions...\")\n",
        "\n",
        "# Get AI suggestions\n",
        "ai_suggestions = ai_cleaning_agent.get_ai_cleaning_suggestions(df, analysis)\n",
        "\n",
        "print(f\"\\nðŸ’¡ AI Generated {len(ai_suggestions)} Intelligent Suggestions:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, suggestion in enumerate(ai_suggestions, 1):\n",
        "    priority_emoji = \"ðŸ”´\" if suggestion['priority'] == 'high' else \"ðŸŸ¡\" if suggestion['priority'] == 'medium' else \"ðŸŸ¢\"\n",
        "    print(f\"{i}. {priority_emoji} {suggestion['action']}\")\n",
        "    print(f\"   Reason: {suggestion['reason']}\")\n",
        "    print(f\"   Priority: {suggestion['priority'].title()}\")\n",
        "    print()\n",
        "\n",
        "print(\"ðŸŽ¯ These suggestions will guide our intelligent cleaning process!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 13: Intelligent Data Cleaning\n",
        "print(\"ðŸ§¹ Starting Intelligent Data Cleaning Process...\")\n",
        "\n",
        "# Store original data for comparison\n",
        "df_original = df.copy()\n",
        "analysis_original = analysis.copy()\n",
        "\n",
        "# Perform AI-powered intelligent cleaning\n",
        "df_cleaned = ai_cleaning_agent.intelligent_clean(\n",
        "    df, \n",
        "    follow_ai_suggestions=True,\n",
        "    custom_actions=['optimize_types', 'clean_outliers']\n",
        ")\n",
        "\n",
        "# Analyze the cleaned data\n",
        "analysis_cleaned = cleaning_agent.analyze_data_quality(df_cleaned)\n",
        "\n",
        "print(f\"\\nâœ… Data Cleaning Completed!\")\n",
        "print(f\"ðŸ“Š Original: {df_original.shape[0]:,} rows Ã— {df_original.shape[1]} columns\")\n",
        "print(f\"ðŸ“Š Cleaned: {df_cleaned.shape[0]:,} rows Ã— {df_cleaned.shape[1]} columns\")\n",
        "print(f\"ðŸ“Š Rows removed: {df_original.shape[0] - df_cleaned.shape[0]:,}\")\n",
        "print(f\"ðŸ“Š Missing values cleaned: {sum(analysis_original['missing_values'].values()) - sum(analysis_cleaned['missing_values'].values()):,}\")\n",
        "print(f\"ðŸ“Š Duplicates removed: {analysis_original['duplicate_rows'] - analysis_cleaned['duplicate_rows']:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 14: Before vs After Comparison\n",
        "print(\"ðŸ“Š Creating Before vs After Comparison...\")\n",
        "\n",
        "# Create comprehensive before/after comparison\n",
        "ui.compare_before_after(df_original, df_cleaned, analysis_original, analysis_cleaned)\n",
        "\n",
        "print(\"âœ… Before/After comparison completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 15: Generate Comprehensive Cleaning Report\n",
        "print(\"ðŸ“‹ Generating Comprehensive Cleaning Report...\")\n",
        "\n",
        "# Generate AI-enhanced report\n",
        "report = ai_cleaning_agent.generate_ai_report()\n",
        "\n",
        "print(\"ðŸ“„ AI-Enhanced Data Cleaning Report\")\n",
        "print(\"=\" * 50)\n",
        "print(report)\n",
        "\n",
        "# Save the report to a file\n",
        "with open('data_cleaning_report.md', 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nðŸ’¾ Report saved to 'data_cleaning_report.md'\")\n",
        "print(\"âœ… Comprehensive report generated successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 16: Download Cleaned Data\n",
        "print(\"ðŸ’¾ Preparing Cleaned Data for Download...\")\n",
        "\n",
        "# Save cleaned data to Excel file\n",
        "output_filename = 'cleaned_data.xlsx'\n",
        "df_cleaned.to_excel(output_filename, index=False)\n",
        "\n",
        "print(f\"âœ… Cleaned data saved to '{output_filename}'\")\n",
        "print(f\"ðŸ“Š Final dataset: {df_cleaned.shape[0]:,} rows Ã— {df_cleaned.shape[1]} columns\")\n",
        "\n",
        "# Display sample of cleaned data\n",
        "print(f\"\\nðŸ“‹ Sample of Cleaned Data (First 5 rows):\")\n",
        "print(df_cleaned.head())\n",
        "\n",
        "# For Google Colab users - enable download\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(f\"\\nðŸ“¥ Download your cleaned data:\")\n",
        "    files.download(output_filename)\n",
        "    files.download('data_cleaning_report.md')\n",
        "    print(\"âœ… Files downloaded successfully!\")\n",
        "except ImportError:\n",
        "    print(f\"\\nðŸ’¡ Files saved locally:\")\n",
        "    print(f\"   â€¢ {output_filename} - Your cleaned dataset\")\n",
        "    print(f\"   â€¢ data_cleaning_report.md - Detailed cleaning report\")\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Data Cleaning Process Complete!\")\n",
        "print(f\"ðŸš€ Your data is now clean and ready for analysis!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ **Demo Complete!**\n",
        "\n",
        "### **What You've Accomplished:**\n",
        "\n",
        "âœ… **Loaded and Analyzed** your dataset (WHO Data.xlsx or sample data)  \n",
        "âœ… **Performed Comprehensive** data quality analysis  \n",
        "âœ… **Generated AI-Powered** cleaning suggestions  \n",
        "âœ… **Applied Intelligent** data cleaning techniques  \n",
        "âœ… **Created Beautiful** visualizations and comparisons  \n",
        "âœ… **Generated Detailed** cleaning reports  \n",
        "âœ… **Downloaded Clean** data ready for analysis  \n",
        "\n",
        "### **Key Features Demonstrated:**\n",
        "\n",
        "ðŸ¤– **AI-Powered Intelligence** - Smart cleaning suggestions  \n",
        "ðŸ“Š **Multi-Sheet Excel Support** - Handles complex files  \n",
        "ðŸ” **Comprehensive Analysis** - Detailed data quality insights  \n",
        "ðŸ“ˆ **Visual Comparisons** - Before/after dashboards  \n",
        "ðŸ“‹ **Professional Reporting** - Detailed cleaning logs  \n",
        "ðŸŽ¯ **Real-World Application** - Works with health data  \n",
        "\n",
        "### **Perfect for Competition Demo!**\n",
        "\n",
        "This notebook showcases advanced data cleaning capabilities that are:\n",
        "- **Production-Ready** - Handles real datasets\n",
        "- **AI-Enhanced** - Uses OpenAI for intelligent suggestions  \n",
        "- **User-Friendly** - Clear step-by-step process\n",
        "- **Comprehensive** - Covers all aspects of data cleaning\n",
        "- **Professional** - Generates detailed reports and visualizations\n",
        "\n",
        "**ðŸš€ Ready to impress the judges!**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
